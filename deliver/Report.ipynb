{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Gene Clustering Summary\n",
    "\n",
    "This document contains a summary and current specification of the model we are using for clustering gene expression time series.\n",
    "\n",
    "## Contents\n",
    "1. Prior Work\n",
    "2. Current Model\n",
    "3. What We're doing\n",
    "4. Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Work\n",
    "This will get filled out more in the future, reread papers, and do lit search\n",
    "\n",
    "- HMM mixture model: Train a mixutre of HMMs, extract clustes by assigning each sequence to a generating HMM\n",
    "- Hidden state trajectory clustering: train one HMM on all the genes, cluster based on optimal hidden state sequence\n",
    "- Infinite state HMM clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "From the MyConnectome Project, we have RNA-Seq data from 48 time points collected over the course of roughly 18 months. We choose to omit 5 of the samples due to low RIN values.\n",
    "\n",
    "For the purposes of our model each gene's expression profile is centered and scaled to unit variance. The values observed by our model are a measure of relative over or under expression with respect to an individual genes average epxression level over the course of the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model\n",
    "\n",
    "The basic idea is that given some state space with transitions $\\phi$ and emissions distributions $\\theta$ we would like to find a group of sequences over the state space generating our observed data. In this way we would like to describe how all of our observed gene sequences were generated from a number of cannonical hidden state trajectories.\n",
    "\n",
    "Intuitively we want to capture some underlying biological processes and their corresponding gene expression trajectories. Under this model a gene is a participant/member in some unseen process/cluster. So each gene is a noisy observation of that process.\n",
    "\n",
    "We have observed that if we try to cluster by optimal path each gene tends to go to its own unique optimal path and so we do not achieve a clustering. Identirying a smaller number of underlying hidden state sequences to generate the data is an attempt to resolve the sensitivity of clustering on optimal paths of individual gene sequences.\n",
    "\n",
    "The model differs from the mixture of HMMs approach in that there is one underlying state set and one transition structure. Perhaps a similar approach could be taken in the mixture of HMMs context by having shared state-space, or doing some sort of shared learning in the transition structure of the mixture components (haven't considered this fully)\n",
    "\n",
    "The model differs from both state-sequence clustering and HMM mixture clustering in that the way we evaluate the probability of a cluster only counts the transitions once. We are picking our trajectories, and then say that all the genes in the cluster associated with that trajectory are generated by it. Perhaps this is not a good thing-- in clusters with many genes the total liklihood of the cluster will be dominated by the conditional liklihood of the genes sequences given the trajectory.\n",
    "\n",
    "![model diagram](../figures/fixed_hmm.png)\n",
    "\n",
    "\n",
    "The dependence $T$ has on $\\phi$ has to do with how probable a transition from states $T_t \\rightarrow T_{t+1}$ is.\n",
    "\n",
    "Expanding T and y into the sequences they represent we have:\n",
    "![epanded model diagram](../figures/fixed_hmm_expand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What We're Actually Doing\n",
    "\n",
    "### 1. Train the HMM\n",
    "\n",
    "We train an N state HMM with univariate Gaussian emissions.\n",
    "\n",
    "$ M = (\\phi, \\theta) $ where\n",
    "\n",
    "Each $ \\phi_i,  i = 1, 2, ..., N $   multinomial distribution over states from state i\n",
    "\n",
    "Each $ \\theta_i  \\; \\tilde \\quad N(\\mu_i, \\sigma^2_i), i = 1, 2, ..., N $ univariate gaussian emissions for state i\n",
    "\n",
    "We learn:\n",
    "- transition probabilitites $\\phi_{i,j}$\n",
    "- mean $\\mu_i$ and variance $\\sigma^2_i$ and variance for eat emissions distribution\n",
    "\n",
    "In this case the model generates each gene's expression trajectory independently of each other gene. The model parameters here are the ones that maximize the likelihood of generating all of our observed sequences.\n",
    "\n",
    "This normally goes without saying. But what we are looking for is a model that generates multiple observed sequences as multiple observations of the same underlying process.\n",
    "\n",
    "Once these parameters are learned they are fixed. **These HMM parameters are not updated at any point during the clustering**. In the final version of this model I don't think it makes sense to keep these parameters fixed. We've just been keeping them fixed for now while we figure out the merging/clustering scheme.\n",
    "\n",
    "\n",
    "Learned transition matrix we have for a 3 state HMM looks like this. The general trend seems to be \"Move towards the middle and stay there\"\n",
    "![3-state transition heatmap](../figures/2017-02-20-kt-3n_transition_heatmap.png)\n",
    "\n",
    "Densities for the learned distributions. There is a lof of overlap here. How would this look different if we fixed the variances to some relatively small number? Having a lot of overlap in the emissions distributions will cause us to care more about having likely transitions in a hidden state sequence.\n",
    "![3-state transition heatmap](../figures/2017-02-20-kt-3n-distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agglomerative Clustering\n",
    "\n",
    "Our goal here is to pick a set of hidden state trajectories that generate all of our observed data. To do this we are employing an agglomerative clustering scheme on the Viterbi path of each cluster.\n",
    "\n",
    "#### I. Viterbi Path\n",
    "For a given observation sequence $y = (y_1, y_2, ..., y_T)$, the Viterbi path is the hidden state sequence $x = (x_1, x_2, ..., x_T)$ maximimzing\n",
    "$$\n",
    "P(x, y) = P(x_1)P(x_2 \\,|\\, x1) ... P(x_T \\, |\\, x_{T-1})P(y_1 \\,|\\, x_1) P(y_2 \\,|\\, x_2) ... P(y_T \\,|\\, x_T) \\quad (1)\n",
    "$$\n",
    "\n",
    "#### II. Viterbi Path with multiple observations\n",
    "**This requires attention, not sure if this is a good way to go about what we want.**\n",
    "\n",
    "So our HMM $M$ generates individual gene expression sequences, but we are trying to use it to find groups of genes generated by the hidden state trajectory. Consider a group of observed sequences $S = \\{y^{(1)}, y^{(2)}, ..., y^{(k)}\\}$ We've been computing the probability of a hidden state sequence $x$ as:\n",
    "$$\n",
    "P(S, x) = P(x_1)P(x_2 \\,|\\, x1) ... P(x_T \\, |\\, x_{T-1})  \\prod_{t=1}^{T}\\prod_{i=1}^{k} P(y_{t}^{(i)}\\, |\\, x_t) \\quad (2)\n",
    "$$\n",
    "\n",
    "Take note that the transitions are only being counted once.\n",
    "\n",
    "If we find the path maximizing $P(S, x)$ as described above what we are essentially doing is finding the Viterbi path for S over a new model $M_k = (\\phi_k, \\theta_k)$, where \n",
    "$$\n",
    "\\phi_k = \\phi \\\\\n",
    "\\theta_{k, i} \\, \\tilde \\quad Guassian([\\mu_i, \\mu_i, \\dots, \\mu_i], \\, \\Sigma ) \\; \\Sigma = \n",
    "\\begin{bmatrix}\n",
    "    \\sigma_i^2 & 0 & 0 & \\dots  & 0 \\\\\n",
    "    0 & \\sigma_i^2 & 0 & \\dots  & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots  & \\sigma_i^2\n",
    "\\end{bmatrix}\n",
    "\\quad (3)\n",
    "$$\n",
    "\n",
    "Where $\\mu_i$ and $\\sigma_i^2$ correspond to the parameters of the emissions distribution $\\theta_i$ in $M$.\n",
    "\n",
    "$P(S_t \\,|\\, x_t, M_k) \\, = \\, \\prod_{i=1}^{k} P(y_{t}^{(i)}\\, |\\, x_t) \\;$ for some $S$ at time $t$ \n",
    "\n",
    "As Alexis mentioned, the transition structure $\\phi$ learned in $M$ isn't necessarily optimal in this new context where we are trying to describe the generation of our observed set of sequences $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Clustering\n",
    "\n",
    "#### a. Viterbi Path\n",
    "\n",
    "The agglomerative clustering over Viterbi paths has been performed as followed. At each step we consider all possible merges between existing clusters. We select the merge that is the least costly to the liklihood of the clustering overall.\n",
    "\n",
    "$D = \\{y^{(i)}\\}_{1 \\leq i \\leq G} \\;$ be our observed data, where there are $G$ genes.\n",
    "\n",
    "We initialize with each gene sequence is in its own singleton cluster $S_i = \\{y^{(i)}\\}, i = 1, 2, \\dots, G$.\n",
    "\n",
    "Denote the clustering by $C = \\{ S_1, S_2, \\dots, S_G\\}$\n",
    "\n",
    "\n",
    "**While there is more than one cluster:**\n",
    "1. Compute $T_{S_i \\cup S_j}$, the path $x$ maximizing $P(S_i \\cup S_j, \\,x)$  for all $(S_i, S_j) \\in C \\times C$\n",
    "2. Select $ S_u, S_v = argmax_{S_i, S_j} \\; logP(S_i \\cup S_j, T_{S_i,S_j})  - (logP(S_i \\, | \\, T_{S_i}) + logP(S_j \\, | \\, T_{S_j}))  \\\\$\n",
    "3. $C \\cup \\{S_u \\cup S_v\\} \\setminus\\{S_u, S_v\\} $\n",
    "\n",
    "**Depending on the rule we come up with for cutting the tree we'll probably be able to terminate the clustering part sooner than before we build to whole tree. That is still in development.**\n",
    "\n",
    "![vit merge liklihood](../figures/2017-02-20-kt-vit-1500-total.png)\n",
    "\n",
    "Here is an example of what this clustering scheme looks like on a random 1500 genes from the MyConnectome dataset. The liklihood of the clustering here is evaluated as the sum of log liklihoods of the best paths for each cluster. \n",
    "\n",
    "One way we could consider cutting the tree is simply cutting where the liklihood of the data is maximized under our modelling assumption. We get our highest liklihood after merge 1181. Cuttin the tree here gives us a little more that 300 clusters for 1500 genes.\n",
    "\n",
    "How will this work with larger sets of genes? Where is the clustering's viterbi path liklihood maximized for G genes? We need to find this out if we are going to come up with a sensible policy for picking a place to cut the tree into clusters and wheter or not picking the best total viterbi path liklihood across all cluster merges is the best way to pick our clustering.\n",
    "\n",
    "![vit merge liklihood](../figures/2017-02-20-kt-vit-max-prob-cluster-sizes.png)\n",
    "\n",
    "Here is a look at the distribution over cluster sizes. Most of our clusters are pretty small. If we want to extract larger clusters we still need to come up with some critereon for cutting the tree higher.\n",
    "\n",
    "#### b. Marginalize out state sequences\n",
    "\n",
    "Something we were considering earlier was not looking at a particular generating state sequence but simply that all members in a cluster were generated from some identicle state sequence.\n",
    "\n",
    "If I am not mistaken, when we are only counting the transition probabilities once, this task is the same as finding some clustering/partitioning of the data $C = \\{ S_1, S_2, \\dots, S_G=k\\}$ maximizing $\\prod_{i=1}^{k} P(S_i \\,|\\, M_{c_i})$, where $c_i = |S_i|$ and $M_{c_i}$ is an HMM with identicle transition structure to M but draws from the emissions distribution $c_i$ times at each step (as described above). So we have\n",
    "\n",
    "$D = \\{y^{(i)}\\}_{1 \\leq i \\leq G} \\;$ be our observed data, where there are $G$ genes.\n",
    "\n",
    "We initialize with each gene sequence is in its own singleton cluster $S_i = \\{y^{(i)}\\}, i = 1, 2, \\dots, G$.\n",
    "\n",
    "Denote the clustering by $C = \\{ S_1, S_2, \\dots, S_G\\}$\n",
    "\n",
    "\n",
    "**While there is more than one cluster:**\n",
    "1. Compute $P_{i,j} = P(S_i \\cup S_j, \\ | \\; M_{c_{i} + c_{j}})$ where $c_{i} = |S_i|$\n",
    "2. Select $ u, v = argmax_{i, j}\\; logP_{i,j} - (logP(S_i \\; | \\; M_{c_i}) + logP(S_j \\; | \\; M_{c_j})) $\n",
    "3. $C \\cup \\{S_u \\cup S_v\\} \\setminus\\{S_u, S_v\\} $\n",
    "\n",
    "![1500 merge liklihood](../figures/2017-02-20-kt-1500-total.png)\n",
    "\n",
    "If we do this we see that the liklihood of the data under the sequential clusterings is maximized around the 400th merge, giving us around 1100 clusters with 1 or 2 genes per cluster. Clearly this isn't very informative so we'd have to come up with some way of determining a cutoff higher up the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-path-clustering\n",
    "\n",
    "It is an attractive feature of a clustering algorithm to not have to choose k. However, if in the case of the agglomerative clustering described above we are interested extracting larger clusters then we will at least in some indirect way be choosing k.\n",
    "\n",
    "While we are at that, we could just as well skip the agglomerative bit of things and pick a k by normal means. The greedy agglomerative approach may give us some k-clustering but it is not necessarily an optimal k-clustering. We can explore methods for finding good k clusterings.\n",
    "\n",
    "The obvious approach here (noting that we are still working with fixed HMM parameters) is to perform hard EM. Starting with some initial k clustering of the data we iteratively compute the optimal path for each cluster $\\{ T_1, \\dots T_k\\}$ (M-step) and reassign sequences to the path best describing it (E-step).\n",
    "\n",
    "Note that in the count-the-transitions-once approach the E-step boils down to computing the conditional probability $P(y \\,|\\, x) = P(y_1 \\,|\\, x_1) P(y_2 \\,|\\, x_2) ... P(y_T \\,|\\, x_T) \\quad x \\in \\{ T_1, \\dots T_k\\}$\n",
    "\n",
    "So while the hidden state trajectories are computed with regard for transition probabilities, the assignment of a gene sequence to a given trajectory is done based on the conditional probability that the such a hidden state sequence has been generated\n",
    "\n",
    "We are only counting the transitions once, and trajectories $\\{ T_1, \\dots T_k\\}$ for the k clusters are fixed when we are reassigning sequences to clusters. Then, the optimal hard assignment of a sequence to a cluster is the one minimizing the conditional probability of the observed sequence on the trajectory generating that cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts, Reflections, & Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How sensible is it to keep the learned HMM parameters fixed throughout the clustering process?\n",
    "How bad of an assumption is it to say these parameters are fixed? How much can we expect the model to improve by reistimating transition and/or emissions distributions.\n",
    "\n",
    "Ultimately I think this model will be include estimating transition and/or emissions parameters. I feel like the only reason we are keeping them fixed for now is that 1) the idea for this model came out of working in the HMM state-sequence clustering context, and 2) so that we can focus on figuring out what the rest of the model is doing in terms of how trajectories are generated and how gene expression sequences are generated from those trajectories.\n",
    "\n",
    "Now, I think we should also go back and look at what the emissions distributions are exactly. In the HMM they are just univariate Normal. However, the $\\theta$ written in the diagram is not this. The $\\theta$ in the model supports multiple observations at once. Replicating the HMM to produce differently dimensioned outputs seems kind of hacky, and as Alexis mentions the parameters learned for the univariate case are not necessarily optimal in the higher dimensional cases.\n",
    "\n",
    "Is there a way to have a multivariate normal distribution of random dimension? What should the covariance matrix look like? In the above we observe each data point in a cluster at a given time point as an independent observation of a state in that trajectory. This area needs more thought.\n",
    "\n",
    "Supposing we had randomly dimensioned multivariate normals as our emissions distribution, how do we paramaterize the distribution over this dimension. This would also essentially be a distribution over cluster sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
